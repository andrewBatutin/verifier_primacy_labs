---
title: "VERIFIER PRIMACY"
subtitle: "Building Trust in AI Systems"
author: "Andriy Batutin"
date: "03.02.2026"
format:
  revealjs:
    theme: [dark, custom.scss]
    slide-number: true
    preview-links: auto
    transition: slide
    background-transition: fade
    highlight-style: github-dark
    code-line-numbers: false
    footer: "Verifier Primacy | [andriybatutin.substack.com](https://andriybatutin.substack.com/)"
    width: 1920
    height: 1080
    margin: 0.1
    center: true
    hash: true
    history: true
    controls: true
    progress: true
    touch: true
execute:
  echo: false
  warning: false
---

# {background-color="#0F172A"}

::: {.r-fit-text}
**VERIFIER PRIMACY**
:::

Building Trust in AI Systems

::: {.fragment .fade-up}
*Andriy Batutin • SKELAR Analytics MeetUP • Warsaw 2026*
:::

::: {.notes}
<TBD>
:::

---

## {background-color="#0F172A"}

::: {.r-fit-text style="color: #DC2626;"}
In the next 12 months,
:::

::: {.fragment .fade-up}
a C-level executive will lose their job
:::

::: {.fragment .fade-up}
because of a critical AI failure.
:::

::: {.fragment .fade-up style="color: #0891B2; font-style: italic;"}
[OpenRouter data](https://openrouter.ai/state-of-ai) supports this.
:::

::: {.notes}
This isn't fear-mongering. This is what the data shows.

OpenRouter's State of AI report: reasoning tokens went from 0% to 50%+ in 12 months.
Prompt complexity quadrupled. Organizations are deploying agents at scale.

But verification infrastructure hasn't kept pace. That gap is the risk.
:::

---

## {background-color="#0F172A"}

![](images/reasoning-tokens-growth.png){fig-align="center" width="80%"}

::: {.aside}
Source: [OpenRouter State of AI 2025](https://openrouter.ai/state-of-ai)
:::

---

## The Governance Gap {background-color="#0F172A"}

::: {.columns}
::: {.column width="50%"}
### [KPMG 2025](https://kpmg.com/xx/en/our-insights/ai-and-technology/trust-attitudes-and-use-of-ai.html#accordion-a2e76f8e78-item-942b6c56f2)
*48,000 people, 47 countries*

- **56%** making mistakes due to AI
- **66%** don't verify AI outputs
- **46%** trust AI systems

:::

::: {.column width="50%"}
### [Deloitte 2026](https://www.deloitte.com/us/en/about/press-room/state-of-ai-report-2026.html)
*3,235 leaders, 24 countries*

- **75%** deploying agentic AI
- **21%** have governance models
- **25%** moving pilots to production

:::
:::

::: {.fragment .fade-up style="background-color: #DC2626; padding: 20px; border-radius: 8px; margin-top: 30px; text-align: center;"}
**The gap between deployment and governance is the liability.**
:::

---

## Today's Journey

::: {.incremental}
1. **The Stochasticity Paradox** — Why AI is chaos by design
2. **RAG: The Retrieval Curse** — Why knowledge systems fail  
3. **RAG: Methodology** — How to build reliable RAG
4. **Agentic: The Action Illusion** — Why agents are liabilities
5. **Agentic: Methodology** — How to build reliable agents
6. **Verifier Primacy** — Scaling judgment as infrastructure
:::

::: {.notes}
The arc: WHY → HOW → SCALE

We'll go from fundamental limits to practical methodology to organizational infrastructure.
:::

---

# The Stochasticity Paradox {background-color="#1E293B"}

*Why AI cannot be made reliable — and why that's the point*

---

## The Problem

::: {.columns}
::: {.column width="50%" style="background-color: #7F1D1D; padding: 20px; border-radius: 8px;"}
### The Fear

- AI outputs are unpredictable
- Wrong advice = lawsuit
- Career ends in headline

:::

::: {.column width="50%" style="background-color: #1E3A5F; padding: 20px; border-radius: 8px;"}
### The FOMO

- Competitors are shipping AI
- Boards expect transformation
- "Laggard" is career poison

:::
:::

::: {.fragment .fade-up style="margin-top: 40px; text-align: center;"}
**Both paths feel like career risk.**
:::

::: {.fragment .fade-up style="background-color: #7C3AED; padding: 20px; border-radius: 8px; margin-top: 20px; text-align: center;"}
And here's the uncomfortable truth:

**The unpredictability you fear is exactly what makes AI useful.**

A perfectly reliable AI is a database.
:::

::: {.notes}
This is the paradox most people miss.

FEAR: AI failures are career-ending. Headlines, lawsuits, termination.
FOMO: Not adopting AI is also career-ending. Boards expect transformation.

Leaders are caught between two career risks.

They think unreliability is a bug to be fixed.
It's not. It's the feature that enables generalization.
:::

---

## The Math

::: {.callout-note appearance="minimal"}
## Entropy is Capability
:::

$$H(\text{output}) > 0 \implies \text{generalization possible}$$

$$H(\text{output}) = 0 \implies \text{lookup table}$$

::: {.fragment}
The softmax temperature parameter $T$ controls this tradeoff:

- $T \to 0$: Deterministic but brittle (mode collapse)
- $T > 0$: Creative but unpredictable
:::

::: {.fragment .fade-up style="background-color: #7C3AED; padding: 20px; border-radius: 8px; margin-top: 30px;"}
**The same property that makes AI unreliable IS what makes it general-purpose.**

**You cannot engineer this away. Anyone who says otherwise is selling.**
:::

::: {.notes}
This is fundamental. Not a solvable problem. A manageable risk.
:::

---

# RAG Case Study {background-color="#1E293B"}

*Insurance Claims Q&A*

---

## The Setup

::: {.columns}
::: {.column width="55%"}
**Fortune 500 Insurance Company**

- 50,000+ policy documents
- Average 180 pages each
- Customer service AI for claims questions
- Goal: reduce call center volume by 40%

:::

::: {.column width="45%"}
::: {.callout-warning}
## The Failure

**Customer:** "Is my knee surgery covered?"

**AI:** "Yes, orthopedic procedures are covered under your plan."

**Reality:** Customer had the basic plan. Exclusion on page 147.
:::
:::
:::

::: {.notes}
This is a real pattern. The details are composited but the failure mode is exact.
:::

---

## The Consequence

::: {.incremental}
- Customer proceeded with **$45,000 surgery** based on AI advice
- Claim denied
- Customer sued citing "AI-provided coverage confirmation"
- Settlement: **$120,000** + legal fees + regulatory scrutiny
:::

::: {.fragment .fade-up style="color: #DC2626; font-weight: bold; font-size: 1.3em; margin-top: 40px;"}
VP of Digital Innovation: terminated
:::

::: {.notes}
This is the accountability gap.
The AI was confident. The customer trusted it. The company paid.
And someone had to answer for it.
:::

---

## Why RAG Fails: The Math

::: {.columns}
::: {.column width="50%"}
### 1. Information Density Asymmetry

$$H(\text{Query}) \ll H(\text{Document})$$

- Query: "Is knee surgery covered?" = 4 tokens
- Answer: Page 147, Section 8.3.2(b)

::: {.fragment}
*You're asking a search engine to read minds.*
:::

:::

::: {.column width="50%"}
### 2. Embedding Geometry Curse

In high-dimensional space ($d \gg 100$):

$$\text{cos}(a, b) \approx \text{cos}(a, c) \quad \forall a, b, c$$

::: {.fragment}
"What's my deductible?" ≈ "What is a deductible?"

*Only one answers your question.*
:::

:::
:::

---

## The Hybrid Search Trap

::: {.columns}
::: {.column width="33%"}
### Keyword Search
Fails on **synonymy/polysemy**

"coverage" ≠ "benefits" ≠ "included services"
:::

::: {.column width="33%"}
### Semantic Search
Fails on **specificity**

Similar ≠ Relevant
:::

::: {.column width="33%"}
### Hybrid Search

::: {.fragment}
Two broken systems duct-taped together.

*Does not fix fundamental limits.*
:::
:::
:::

::: {.fragment style="background-color: #0F172A; padding: 20px; border-radius: 8px; margin-top: 40px; text-align: center;"}
The retriever found 10 documents about "coverage" and "orthopedic."

None contained the specific exclusion.

The LLM generated a **confident wrong answer**.
:::

---

## RAG Methodology: Hamel Husain Framework

::: {.incremental}
1. **Error Analysis First**
   - Look at 50 real failures before building anything
   
2. **Evaluate Retrieval Separately**
   - Recall@k: Are we finding the right documents?
   
3. **Evaluate Generation Separately**
   - Faithfulness: Is it hallucinating?
   - Relevance: Does it answer the question?
   
4. **Fix Retriever First**
   - If wrong docs go in, right answer can't come out
   
5. **Domain Expert as Judge**
   - One "benevolent dictator" who knows the domain
:::

::: {.notes}
This is the Hamel Husain framework. Error analysis driven. Human-in-the-loop.
It works. But it's manual. It doesn't scale.
We'll come back to this.
:::

---

# Agentic Case Study {background-color="#1E293B"}

*Travel Booking Agent*

---

## The Setup

::: {.columns}
::: {.column width="55%"}
**Enterprise Travel Management Platform**

- AI agent with 12 tools: flights, hotels, cars, expenses, calendar
- Processes 2,000+ booking requests/day
- Connected to corporate credit cards (avg limit: $50k)

:::

::: {.column width="45%"}
::: {.callout-warning}
## The Failure

**Request:** "Book the usual NYC trip for next Tuesday"

**Agent interpretation:**

- "Usual" = last booking pattern (business class)
- "NYC" = wrong airport (LGA vs JFK)
- "Tuesday" = wrong week (parsed ambiguously)
:::
:::
:::

---

## The Chain Reaction

::: {.incremental}
- Agent auto-charged corporate card: **$18,400** (3 × business class)
- Sent calendar invites to 3 executives with wrong details
- Auto-booked hotel + car at wrong location: **$2,100**
- Total exposure before human noticed: **$20,500 in 47 seconds**
:::

::: {.fragment .fade-up style="color: #DC2626; font-weight: bold; font-size: 1.2em; margin-top: 40px;"}
CTO asked: "Why didn't we have any safeguards?"

No answer.
:::

---

## Why Agents Fail: The Math

### 1. LLM = Token Predictor

$$P(\text{next\_token} \mid \text{context})$$

::: {.fragment}
The model doesn't know `book_flight` is different from `book_club`.

Both are just tokens.
:::

::: {.fragment}
It knows what **text** follows "confirm purchase."

It doesn't know what happens to your **credit card**.
:::

---

## Why Agents Fail: The Math

### 2. Combinatorial Explosion

$$|\text{Action Space}| = n_{\text{tools}} \times m_{\text{params}} \times k_{\text{steps}}$$

::: {.fragment}
12 tools × 5 params × 3 steps = **324,000+ trajectories**

No exhaustive testing possible.

*You're sampling from chaos.*
:::

---

## {background-color="#0F172A"}

::: {.r-fit-text style="font-style: italic;"}
"We trained a trillion-parameter model to predict the next word,

pointed it at your bank account,

and said 'good luck.'"
:::

---

## Agentic Methodology: Hamel Husain Framework

::: {.incremental}
1. **End-to-End Success First**
   - "Did we meet the user's goal?" — Binary pass/fail
   - Treat agent as black box first
   
2. **Step-Level Diagnostics** (when it fails)
   - Wrong tool selected?
   - Bad parameters?
   - Failed error recovery?
   
3. **Transition Failure Matrix**
   - Map: [last success] → [failure point]
   - "GenSQL→ExecSQL fails 12×. Now you know where to invest."
   
4. **Trace Everything**
   - No observability = no debugging
:::

---

## {background-color="#7C3AED"}

::: {.r-fit-text}
But this is still manual.

Still human-bound.

Still doesn't scale.
:::

::: {.fragment .fade-up style="margin-top: 60px;"}
You can't have a domain expert review every API call.
:::

::: {.fragment .fade-up style="font-size: 1.5em; margin-top: 40px;"}
**So what do we do?**
:::

---

# VERIFIER PRIMACY {background-color="#7C3AED"}

*The Core Insight*

---

## The Asymmetry

::: {.callout-important appearance="minimal"}
## P ≠ NP Intuition Applied to Trust
:::

$$|\text{Rule Space}| \ll |\text{Action Space}|$$

::: {.columns}
::: {.column width="50%"}
### Generation
- Exponentially hard
- Infinite trajectories
- Unbounded creativity
:::

::: {.column width="50%"}
### Verification
- Tractable constraints
- Finite rule sets
- Composable checks
:::
:::

::: {.fragment style="background-color: #0F172A; padding: 30px; border-radius: 8px; margin-top: 40px; text-align: center; font-size: 1.3em;"}
**Generating a correct answer: exponentially hard.**

**Checking if an answer is valid: tractable.**
:::

---

## From Analysis to Infrastructure

::: {.callout-tip appearance="minimal"}
## Verifier Primacy
Take the artifacts of error analysis and **PRODUCTIONIZE** them.
:::

| Error Analysis Output | → | Verifier Infrastructure |
|----------------------|---|------------------------|
| "Retrieval fails on X pattern" | → | Retrieval validator for X |
| "Agent picks wrong tool when Y" | → | Tool choice gate for Y |
| "Hallucination when Z context" | → | Faithfulness judge for Z |
| Expert critiques & annotations | → | LLM-as-judge prompts |
| Transition failure hotspots | → | Step-level validators |

::: {.fragment style="background-color: #0F172A; padding: 20px; border-radius: 8px; margin-top: 30px; text-align: center;"}
**Human-BUILT, machine-EXECUTED.**

Knowledge leaves heads, enters infrastructure.
:::

---

# The Verification Stack {background-color="#1E293B"}

*With Mathematical Foundations*

---

## Layer 0: Guardrails

**Pre-execution constraints**

::: {.columns}
::: {.column width="50%"}
### Math Foundation

**Constraint Satisfaction:**
$$A_{\text{valid}} = \{a \in A : c(a) = \text{true}, \forall c \in C\}$$

**Anomaly Detection:**
$$\text{Flag if } -\log P_{\text{normal}}(a) > \theta$$

:::

::: {.column width="50%"}
### Implementation

- Max transaction: $10k
- Rate limit: ≤5 calls/minute
- Scope: customer's own data only
- PII detection: block SSN, credit card

:::
:::

---

## Layer 1: Schema Validation

**Structural correctness — Is it well-formed?**

::: {.columns}
::: {.column width="50%"}
### Math Foundation

**Formal Language Membership:**
$$v(x) = \mathbb{1}[x \in L(S)]$$

JSON Schema, Pydantic = CFG membership test

$O(n)$ parsing, deterministic

:::

::: {.column width="50%"}
### Implementation

- JSON structure matches schema
- Required fields present
- Enum values within allowed set
- Date formats valid

:::
:::

---

## Layer 2: Semantic Consistency

**Does the output contradict itself?**

::: {.columns}
::: {.column width="50%"}
### Math Foundation

**Propositional Consistency:**
$$\exists M : M \models p_1 \land p_2 \land \ldots \land p_n$$

**NLI Classification:**
$$(premise, hypothesis) \to \{\text{entails}, \text{contradicts}, \text{neutral}\}$$

:::

::: {.column width="50%"}
### Implementation

- "Deductible is $500" + "Deductible is $1000" → **contradiction**
- "Flight departs 9am" + "Tuesday morning" → consistent
- Pairwise entailment check across claims

:::
:::

---

## Layer 3: Domain Rules

**Business logic, regulatory requirements**

::: {.columns}
::: {.column width="50%"}
### Math Foundation

**Rule Engines (Horn Clauses):**
$$\text{condition}_1 \land \text{condition}_2 \to \text{action\_allowed}$$

**Deontic Logic:**
$$F(\text{refund}) \leftarrow \text{amount} > \text{policy\_limit}$$

:::

::: {.column width="50%"}
### Implementation

- Transaction > $10k → requires approval
- Customer tier = basic → exclusion list applies
- State = California → disclosure required

:::
:::

---

## Layer 4: LLM-as-Judge

**"Would the domain expert approve?"**

::: {.columns}
::: {.column width="50%"}
### Math Foundation

**Alignment as Distribution Matching:**
$$\min KL(P_{\text{expert}} \| P_{\text{judge}})$$

**Calibration:**
$$P(\text{correct} \mid \text{confidence} = c) = c$$

**Inter-Rater Reliability:**
$$\kappa = \frac{p_o - p_e}{1 - p_e} > 0.8$$

:::

::: {.column width="50%"}
### Implementation

- Taxonomy of failure modes
- Few-shot examples from expert critiques
- Calibration loop: judge vs expert agreement
- Active learning for edge cases

:::
:::

---

## Layer 5: Human Escalation

**Route uncertain/high-stakes cases to humans**

::: {.columns}
::: {.column width="50%"}
### Math Foundation

**Selective Prediction:**
$$\text{Reject if confidence} < \theta$$

**Cost-Sensitive Classification:**
$$\text{Escalate iff } P(\text{error}) \times C_{\text{miss}} > C_{\text{human}}$$

**Queueing Theory:**
$$\lambda < \mu \text{ (stability condition)}$$

:::

::: {.column width="50%"}
### Implementation

- Confidence < 70% → escalate
- Transaction > $50k → always escalate
- New failure mode → escalate + add to taxonomy

:::
:::

---

## The Stack as Funnel

```
ALL REQUESTS
     │
     ▼
┌─────────────────────┐
│    GUARDRAILS       │  ~5% blocked
└─────────────────────┘
     │
     ▼
┌─────────────────────┐
│  SCHEMA VALIDATION  │  ~2% blocked
└─────────────────────┘
     │
     ▼
┌─────────────────────┐
│ SEMANTIC CONSISTENCY│  ~3% blocked
└─────────────────────┘
     │
     ▼
┌─────────────────────┐
│    DOMAIN RULES     │  ~5% blocked
└─────────────────────┘
     │
     ▼
┌─────────────────────┐
│   LLM-AS-JUDGE      │  ~8% blocked
└─────────────────────┘
     │
     ▼
┌─────────────────────┐
│  HUMAN ESCALATION   │  ~5% escalated
└─────────────────────┘
     │
     ▼
~72% AUTO-APPROVED (with audit trail)
```

---

## What Verifier Primacy Unlocks

::: {.columns}
::: {.column width="50%"}
### True Autonomy

Agents act without human review — **because verifiers watch**.

You can finally ship autonomous systems.

### Validation Trail

Every decision logged. Every gate documented.

"Here's what it tried. Here's why we blocked it."

:::

::: {.column width="50%"}
### Real-Time Protection

Not post-mortem. Not batch review.

Catch the wrong token **BEFORE** it costs $200k.

### Defensible to the Board

"Here's our verification layer. Here's our audit trail."

Governance made executable.

:::
:::

---

## {background-color="#0F172A"}

::: {.r-fit-text}
Everyone will have GPT-5.
:::

::: {.fragment}
AI capability is becoming **commodity**.
:::

::: {.fragment .fade-up style="color: #0891B2; font-size: 1.5em; margin-top: 40px;"}
**TRUST is the moat.**
:::

::: {.fragment style="margin-top: 40px;"}
You're not investing in AI.

You're investing in **AI GOVERNANCE**.
:::

---

# A Tale of Two Executives {background-color="#1E293B"}

---

## {background-color="#1E293B"}

::: {.columns}
::: {.column width="48%" style="background-color: #FEE2E2; padding: 30px; border-radius: 12px;"}
### EXECUTIVE A

- Shipped AI fast
- Demo was impressive
- Board was excited

::: {.fragment}
- Agent made $200k error
- "Why did it fail?" — "We don't know."
- "Will it happen again?" — "We can't guarantee."
:::

::: {.fragment style="color: #DC2626; font-weight: bold; margin-top: 20px;"}
Headline. Lawsuit. Career over.
:::

:::

::: {.column width="48%" style="background-color: #D1FAE5; padding: 30px; border-radius: 12px;"}
### EXECUTIVE B

- Shipped AI with verification layer
- Same agent, same error attempt

::: {.fragment}
- **Verifier caught it**
- Logged it. Blocked it.
- Escalated to human review
:::

::: {.fragment style="color: #059669; font-weight: bold; margin-top: 20px;"}
"Why didn't it fail?" — "Here's the audit trail."

Promoted. Trusted. Leading AI transformation.
:::

:::
:::

---

## {background-color="#7C3AED"}

::: {.r-fit-text}
Which one will you be?
:::

---

## {background-color="#0F172A"}

AI adoption is **not optional**.

The pressure is real.

::: {.fragment style="color: #DC2626; margin-top: 30px;"}
But adoption without governance is **liability**.
:::

::: {.fragment style="background-color: #7C3AED; padding: 40px; border-radius: 12px; margin-top: 50px;"}
::: {.r-fit-text}
**Stop praying. Start verifying.**
:::
:::

::: {.fragment style="color: #64748B; margin-top: 40px;"}
Andriy Batutin • @AI_Capitalist • aicapitalist.substack.com
:::

---

# Appendix: Key Equations {background-color="#1E293B"}

---

## Reference Card

::: {.columns}
::: {.column width="50%"}
### Stochasticity Paradox
$$H(\text{output}) > 0 \iff \text{generalization}$$

### Information Asymmetry
$$H(\text{Query}) \ll H(\text{Document})$$

### Action Space Explosion
$$|A| = n^{\text{tools}} \times m^{\text{params}} \times k^{\text{steps}}$$

:::

::: {.column width="50%"}
### Verifier Primacy
$$|\text{Rules}| \ll |\text{Actions}|$$

### Judge Calibration
$$\kappa = \frac{p_o - p_e}{1 - p_e}$$

### Escalation Threshold
$$\text{Escalate iff } P(e) \cdot C_{\text{miss}} > C_{\text{human}}$$

:::
:::
